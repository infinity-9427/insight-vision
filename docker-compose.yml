services:
  ollama:
    image: ollama/ollama:latest
    container_name: insightvision-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_KEEP_ALIVE=${OLLAMA_KEEP_ALIVE:-300}
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_DEBUG=${DEBUG:-false}
    deploy:
      resources:
        limits:
          memory: ${OLLAMA_MEMORY_LIMIT:-28g}
          cpus: '${OLLAMA_CPU_LIMIT:-8}'
        reservations:
          memory: ${OLLAMA_MEMORY_RESERVATION:-16g}
          cpus: '${OLLAMA_CPU_RESERVATION:-4}'
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - insightvision-network

  # Model downloader service - pulls required models automatically
  ollama-models:
    image: alpine:latest
    container_name: insightvision-ollama-models
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=http://ollama:11434
      - MODEL_NAME=${MODEL_NAME:-llama3.2:3b-instruct-q8_0}
      - BACKUP_MODEL=${BACKUP_MODEL:-llama3.2:1b}
    depends_on:
      ollama:
        condition: service_healthy
    command: >
      sh -c "
        echo 'üöÄ InsightVision AI - Model Initialization Started'
        echo '======================================================='
        echo 'Primary Model: $${MODEL_NAME}'
        echo 'Backup Model:  $${BACKUP_MODEL}'
        echo 'Ollama Host:   $${OLLAMA_HOST}'
        echo ''
        echo 'ÔøΩ Installing curl for Ollama API calls...'
        apk add --no-cache curl
        echo ''
        echo 'ÔøΩüì• Downloading primary model: $${MODEL_NAME}'
        if curl -X POST $${OLLAMA_HOST}/api/pull -d '{\"name\":\"'$${MODEL_NAME}'\"}' -H 'Content-Type: application/json'; then
          echo '‚úÖ Successfully downloaded primary model: $${MODEL_NAME}'
          echo 'üîÑ Setting up backup model for redundancy...'
          curl -X POST $${OLLAMA_HOST}/api/pull -d '{\"name\":\"'$${BACKUP_MODEL}'\"}' -H 'Content-Type: application/json' || echo '‚ö†Ô∏è  Backup model failed, but primary is available'
        else
          echo '‚ö†Ô∏è  Primary model failed, trying backup model: $${BACKUP_MODEL}'
          if curl -X POST $${OLLAMA_HOST}/api/pull -d '{\"name\":\"'$${BACKUP_MODEL}'\"}' -H 'Content-Type: application/json'; then
            echo '‚úÖ Backup model setup complete!'
          else
            echo '‚ùå Both models failed to download'
            exit 1
          fi
        fi
        echo ''
        echo 'üìã Final model inventory:'
        curl -s $${OLLAMA_HOST}/api/tags | head -20
        echo ''
        echo 'üéØ Model initialization complete!'
        echo 'üöÄ InsightVision AI is ready!'
      "
    restart: "no"
    networks:
      - insightvision-network

  app:
    build: 
      context: ./backend
      dockerfile: Dockerfile
      args:
        - BUILD_ENV=${BUILD_ENV:-development}
    container_name: insightvision-backend
    ports:
      - "${PORT:-8000}:${PORT:-8000}"
    environment:
      # LLM Configuration
      - MODEL_SOURCE=${MODEL_SOURCE:-local}
      - MODEL_NAME=${MODEL_NAME:-llama3.2:3b-instruct-q8_0}
      - BACKUP_MODEL=${BACKUP_MODEL:-llama3.2:1b}
      - GEMINI_MODEL=${GEMINI_MODEL:-gemini-1.5-flash}
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      
      # App Settings
      - PORT=${PORT:-8000}
      - HOST=${HOST:-0.0.0.0}
      - DEBUG=${DEBUG:-false}
      - UPLOAD_DIR=${UPLOAD_DIR:-./uploads}
      - MAX_FILE_SIZE=${MAX_FILE_SIZE:-50}
      - ALLOWED_EXTENSIONS=${ALLOWED_EXTENSIONS:-csv,pdf,xlsx,xls,json}
      - MAX_CONCURRENT_USERS=${MAX_CONCURRENT_USERS:-10}
      
      # Processing
      - MAX_ROWS_PREVIEW=${MAX_ROWS_PREVIEW:-1000}
      - CHART_DPI=${CHART_DPI:-300}
      
      # Ollama Settings
      - OLLAMA_HOST=http://ollama:11434
      - OLLAMA_TIMEOUT=${OLLAMA_TIMEOUT:-120}
      - OLLAMA_KEEP_ALIVE=${OLLAMA_KEEP_ALIVE:-300}
      
      # CORS
      - CORS_ORIGINS=${CORS_ORIGINS:-http://localhost:3000}
      
      # Logging
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      
      # Security
      - JWT_SECRET=${JWT_SECRET}
      - ENABLE_API_RATE_LIMITING=${ENABLE_API_RATE_LIMITING:-true}
      
    volumes:
      - ./backend/uploads:/app/uploads:Z
      - ./backend/data:/app/data:Z
      - ./backend/logs:/app/logs:Z
    depends_on:
      ollama:
        condition: service_healthy
      ollama-models:
        condition: service_completed_successfully
    deploy:
      resources:
        limits:
          memory: ${APP_MEMORY_LIMIT:-4g}
          cpus: '${APP_CPU_LIMIT:-2}'
        reservations:
          memory: ${APP_MEMORY_RESERVATION:-2g}
          cpus: '${APP_CPU_RESERVATION:-1}'
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${PORT:-8000}/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    networks:
      - insightvision-network

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      args:
        - NODE_ENV=${NODE_ENV:-production}
    container_name: insightvision-frontend
    ports:
      - "3000:3000"
    environment:
      - NEXT_PUBLIC_API_URL=${NEXT_PUBLIC_API_URL:-http://localhost:8000}
      - NODE_ENV=${NODE_ENV:-production}
      - PORT=3000
    depends_on:
      app:
        condition: service_healthy
    deploy:
      resources:
        limits:
          memory: ${FRONTEND_MEMORY_LIMIT:-2g}
          cpus: '${FRONTEND_CPU_LIMIT:-1}'
        reservations:
          memory: ${FRONTEND_MEMORY_RESERVATION:-1g}
          cpus: '${FRONTEND_CPU_RESERVATION:-0.5}'
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - insightvision-network

volumes:
  ollama_data:
    driver: local

networks:
  insightvision-network:
    driver: bridge
    name: insightvision-network
